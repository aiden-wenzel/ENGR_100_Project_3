{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tracks(\n",
    "    path_1: str, path_2: str, mult_1=1, mult_2=1, sr=22050\n",
    ") -> Tuple[np.array, int]:\n",
    "    y_1, sr_1 = librosa.load(path_1, sr=sr)\n",
    "    y_2, sr_2 = librosa.load(path_2, sr=sr)\n",
    "    if y_1.size > y_2.size:\n",
    "        y_1 = np.pad(\n",
    "            y_1, (0, sr_1 - (y_1.size % sr_1)), \"constant\", constant_values=(0)\n",
    "        )\n",
    "        y_2 = np.pad(y_2, (0, y_1.size - y_2.size), \"constant\", constant_values=(0))\n",
    "    else:\n",
    "        y_2 = np.pad(\n",
    "            y_2, (0, sr_2 - (y_2.size % sr_2)), \"constant\", constant_values=(0)\n",
    "        )\n",
    "        y_1 = np.pad(y_1, (0, y_2.size - y_1.size), \"constant\", constant_values=(0))\n",
    "    y = np.add(y_1 * mult_1, y_2 * mult_2)\n",
    "\n",
    "    return (y, sr)\n",
    "\n",
    "\n",
    "def DB_spectogram(y: np.array, sr=22050) -> np.array:\n",
    "    return librosa.amplitude_to_db(\n",
    "        librosa.feature.melspectrogram(\n",
    "            y=y.reshape((int)(y.size / sr), sr),  # split into 1-second intervals\n",
    "            hop_length=int(0.0116 * sr),\n",
    "            n_fft=int(0.0464 * sr),\n",
    "            n_mels=96,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def add_gaussian_noise(data, std=0.005):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to an array.\n",
    "    :param data: numpy array of audio data.\n",
    "    :param mean: Mean of the Gaussian noise.\n",
    "    :param std: Standard deviation of the Gaussian noise.\n",
    "    :return: Noisy numpy array.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, std, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "\n",
    "def process_and_save_audio(\n",
    "    files: list, labels: list, output_path: str, sr=22050, add_noise=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a list of audio files and saves the modified numpy arrays.\n",
    "    :param files: List of file paths to the audio files.\n",
    "    :param output_path: Path to save the compressed numpy arrays.\n",
    "    :param add_noise: Boolean indicating whether to add Gaussian noise.\n",
    "    \"\"\"\n",
    "    if len(files) != len(labels):\n",
    "        raise Exception(\"Length of files must equal labels length\")\n",
    "    processed_data = {}\n",
    "\n",
    "    files_to_process = []\n",
    "    labels_to_process = []\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        f = files[i]\n",
    "        l = labels[i]\n",
    "        if os.path.isdir(f):\n",
    "            dir_files = [\n",
    "                os.path.join(dirpath, f)\n",
    "                for (dirpath, _, filenames) in os.walk(f)\n",
    "                for f in filenames\n",
    "            ]\n",
    "            files_to_process.extend(dir_files)\n",
    "            labels_to_process.extend([l] * len(dir_files))\n",
    "        else:\n",
    "            files_to_process.append(f)\n",
    "            labels_to_process.append(l)\n",
    "\n",
    "    data_means = []\n",
    "    data_sizes = []\n",
    "    data_variances = []\n",
    "    \n",
    "    label_size = max(labels) + 1\n",
    "    print(label_size)\n",
    "\n",
    "    for i in range(len(files_to_process)):\n",
    "        for j in range(i + 1, len(files_to_process)):\n",
    "            m, std, sz = process_and_save_audio_double_helper(\n",
    "                file_path=files_to_process[i],\n",
    "                file_path_2=files_to_process[j],\n",
    "                label=labels_to_process[i],\n",
    "                label_2=labels_to_process[j],\n",
    "                processed_data=processed_data,\n",
    "                label_size=label_size\n",
    "            )\n",
    "            data_means.append(m)\n",
    "            data_variances.append(std**2)\n",
    "            data_sizes.append(sz)\n",
    "\n",
    "        m, std, sz = process_and_save_audio_single_helper(\n",
    "            file_path=files_to_process[i],\n",
    "            label=labels_to_process[i],\n",
    "            processed_data=processed_data,\n",
    "            label_size=label_size\n",
    "        )\n",
    "        data_means.append(m)\n",
    "        data_variances.append(std**2)\n",
    "        data_sizes.append(sz)\n",
    "\n",
    "    overall_mean = sum(mean * size for mean, size in zip(data_means, data_sizes)) / sum(\n",
    "        data_sizes\n",
    "    )\n",
    "    overall_stddev = np.sqrt(\n",
    "        (\n",
    "            sum(\n",
    "                (size - 1) * variance\n",
    "                for size, variance in zip(data_sizes, data_variances)\n",
    "            )\n",
    "            + sum(\n",
    "                size * (mean - overall_mean) ** 2\n",
    "                for size, mean in zip(data_sizes, data_means)\n",
    "            )\n",
    "        )\n",
    "        / (sum(data_sizes) - 1)\n",
    "    )\n",
    "\n",
    "    for k in processed_data:\n",
    "        if \"_data\" in k:\n",
    "            processed_data[k] = (processed_data[k] - overall_mean) / overall_stddev\n",
    "\n",
    "    processed_data[\"overall_metadata\"] = np.array([overall_mean, overall_stddev])\n",
    "\n",
    "    # Save all processed data to a compressed numpy file\n",
    "    np.savez_compressed(output_path, **processed_data)\n",
    "\n",
    "\n",
    "def process_and_save_audio_single_helper(\n",
    "    file_path: str, label:int, label_size:int, processed_data: dict, sr=22050, add_noise=False\n",
    "):\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    audio = np.pad(audio, (0, sr - (audio.size % sr)), \"constant\", constant_values=(0))\n",
    "\n",
    "    # Optionally add Gaussian noise\n",
    "    if add_noise:\n",
    "        audio = add_gaussian_noise(audio)\n",
    "\n",
    "    y = DB_spectogram(audio)\n",
    "\n",
    "    key = get_file_subdir_and_name(file_path)\n",
    "    \n",
    "    labels = [0]*label_size\n",
    "    labels[label] = 1\n",
    "\n",
    "    # Add to the dictionary with the constructed key\n",
    "    for i in range(y.shape[0]):\n",
    "        processed_data[key + \"_data\" + f'_{i}'] = y[i]\n",
    "    processed_data[key + \"_metadata\"] = np.array([np.mean(y), np.std(y), y.size])\n",
    "    processed_data[key + \"_labels\"] = np.array(labels)\n",
    "\n",
    "    return np.mean(y), np.std(y), y.size\n",
    "\n",
    "\n",
    "def process_and_save_audio_double_helper(\n",
    "    file_path: str,\n",
    "    file_path_2: str,\n",
    "    label:int,\n",
    "    label_2:int,\n",
    "    label_size:int,\n",
    "    processed_data: dict,\n",
    "    sr=22050,\n",
    "    add_noise=False,\n",
    "):\n",
    "    audio, _ = combine_tracks(file_path, file_path_2, sr=sr)\n",
    "\n",
    "    # Optionally add Gaussian noise\n",
    "    if add_noise:\n",
    "        audio = add_gaussian_noise(audio)\n",
    "\n",
    "    y = DB_spectogram(audio)\n",
    "\n",
    "    key = (\n",
    "        get_file_subdir_and_name(file_path)\n",
    "        + \"_\"\n",
    "        + get_file_subdir_and_name(file_path_2)\n",
    "    )\n",
    "\n",
    "    # Add to the dictionary with the constructed key\n",
    "    for i in range(y.shape[0]):\n",
    "        processed_data[key + \"_data\" + f'_{i}'] = y[i]\n",
    "    processed_data[key + \"_metadata\"] = np.array([np.mean(y), np.std(y), y.size])\n",
    "    \n",
    "    labels = [0]*label_size\n",
    "    labels[label] = 1\n",
    "    labels[label_2] = 1\n",
    "    \n",
    "    processed_data[key + \"_labels\"] = np.array(labels)\n",
    "\n",
    "    return np.mean(y), np.std(y), y.size\n",
    "\n",
    "\n",
    "def get_file_subdir_and_name(file_path: str) -> str:\n",
    "    # Construct a key from the file path to include subdirectory\n",
    "    file_path_parts = file_path.split(os.sep)\n",
    "    if len(file_path_parts) > 1:\n",
    "        name = os.sep.join(\n",
    "            file_path_parts[-2:]\n",
    "        )  # Last two parts: subdirectory and file name\n",
    "    else:\n",
    "        name = file_path_parts[0]  # Only the file name, no subdirectory\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def load_npz_file_with_condition(file_path, max_size: int):\n",
    "    \"\"\"\n",
    "    Loads an .npz file. If the file is over 1GB, it uses mmap_mode='r'.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: The path to the .npz file.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary-like object with lazy loading for large files or directly loaded data for smaller files.\n",
    "    \"\"\"\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    if file_size > max_size:\n",
    "        print(f\"File size is {file_size / (1024**2):.2f}MB. Using mmap_mode='r'.\")\n",
    "        data = np.load(file_path, mmap_mode=\"r\", allow_pickle=True)\n",
    "    else:\n",
    "        print(f\"File size is {file_size / (1024**2):.2f}MB. Loading normally.\")\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "y, sr = combine_tracks(\n",
    "    \"sample_audio_training/trumpet/0005.wav\", \"sample_audio_training/violin/0010.wav\"\n",
    ")\n",
    "db_spec = DB_spectogram(y, sr)\n",
    "\n",
    "\n",
    "files = [\"sample_audio_training/violin\", \"sample_audio_training/oboe\", \"sample_audio_training/trumpet\"]\n",
    "output_path = \"processed_audio.npz\"\n",
    "process_and_save_audio(files, [0, 1, 2], output_path, add_noise=True)\n",
    "db_data = load_npz_file_with_condition(output_path, 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in db_data:\n",
    "    print(k, \"\\n\", db_data[k].shape)\n",
    "    if \"labels\" in k:\n",
    "        print(db_data[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_data[\"violin/0002.wav_trumpet/0000.wav_data_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Mel spectrogram and the PCEN-normalized spectrogram\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 8))\n",
    "\n",
    "img1 = librosa.display.specshow(\n",
    "    db_data[\"violin/0002.wav_violin/0003.wav_data_1\"],\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"mel\",\n",
    "    sr=sr,\n",
    "    hop_length=int(0.0116 * sr),\n",
    ")\n",
    "\n",
    "fig.colorbar(img1, format=\"%+2.0f dB\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
