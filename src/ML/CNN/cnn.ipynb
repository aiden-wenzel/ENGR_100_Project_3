{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITON\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import ELU\n",
    "from torch import flatten\n",
    "\n",
    "class MultiLabelCNN(Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiLabelCNN, self).__init__()\n",
    "        \n",
    "        self.hidden_layers = Sequential (\n",
    "            Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3)),\n",
    "            BatchNorm2d(64),\n",
    "            ELU(),\n",
    "            MaxPool2d((2,2)),\n",
    "\n",
    "            Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3)),\n",
    "            BatchNorm2d(128),\n",
    "            ELU(),\n",
    "            MaxPool2d((2, 2)),\n",
    "\n",
    "            Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3)),\n",
    "            BatchNorm2d(256),\n",
    "            ELU(),\n",
    "            MaxPool2d((3, 3)),\n",
    "\n",
    "            Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3)),\n",
    "            BatchNorm2d(256),\n",
    "            ELU(),\n",
    "            MaxPool2d((3, 3)),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(in_features=256, out_features=128),  # Adjusted input features to match flattened conv output\n",
    "            ELU(),\n",
    "            Linear(in_features=128, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the convolutional layers\n",
    "        x = self.hidden_layers(x)\n",
    "        \n",
    "        # Flatten the output of the convolutional layers to fit linear layer input\n",
    "        x = flatten(x, 1)  # Flatten all dimensions except the batch\n",
    "        \n",
    "        # Pass data through linear layers\n",
    "        x = self.linear_layers(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tvalencia/umich/w24/engr100/project3/src/ML/CNN\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# compress\n",
    "from utils import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "curr = \"../../../sample_audio_training/\"\n",
    "\n",
    "folders = ['oboe', 'trumpet', 'violin']\n",
    "files = []\n",
    "labels = []\n",
    "\n",
    "for folder in folders:\n",
    "    folderPath = os.path.join(curr, folder)\n",
    "    for filename in os.listdir(folderPath):\n",
    "        file_path = os.path.join(folderPath, filename)\n",
    "        if os.path.isfile(file_path):  # Make sure it's a file, not a directory or a symlink\n",
    "            files.append(file_path)\n",
    "            labels.append(folder)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "numeric_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "process_and_save_audio(files=files, labels=numeric_labels, output_path=\"data.npz\", sr=22050, add_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from typing import Tuple\n",
    "import logging, sys\n",
    "import numpy as np\n",
    "\n",
    "def load_npz_file_with_condition(file_path, max_size: int):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    if file_size > max_size:\n",
    "        logging.info(\n",
    "            f\"File size is {file_size / (1024**2):.2f}MB. Using mmap_mode='r'.\"\n",
    "        )\n",
    "        data = np.load(file_path, mmap_mode=\"r\", allow_pickle=True)\n",
    "    else:\n",
    "        logging.info(f\"File size is {file_size / (1024**2):.2f}MB. Loading normally.\")\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, npz_path):\n",
    "        self.data = load_npz_file_with_condition(npz_path, max_size=1024**3)\n",
    "        self.keys = [k for k in self.data.keys() if \"_data\" in k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    # return a 96 x 87 matrix for the CNN\n",
    "    def __getitem__(self, idx):\n",
    "        data_key = self.keys[idx]\n",
    "        data = self.data[data_key]\n",
    "        labels = self.data[f'{data_key.split(\"_data_\")[0]}_labels']\n",
    "        return torch.tensor(data.reshape(1, 96, 87), dtype=torch.float32), torch.tensor(\n",
    "            labels, dtype=torch.float32\n",
    "        )\n",
    "    \n",
    "def train_model(\n",
    "    model, train_dataloader, validation_dataloader, criterion, optimizer, epochs=5\n",
    ") -> Tuple[list, list]:\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logging.info(f\"Epoch {epoch+1}\")\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for data, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            correct_predictions += (predicted == labels).float().sum()\n",
    "            total_predictions += torch.numel(labels)\n",
    "\n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "        logging.info(f\"Loss: {total_loss}\")\n",
    "        logging.info(f\"Train Accuracy: {train_accuracy.item()}\")\n",
    "        train_accuracies.append(train_accuracy.item())\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            for data, labels in validation_dataloader:\n",
    "                outputs = model(data)\n",
    "                predicted = torch.sigmoid(outputs) > 0.5\n",
    "                correct_predictions += (predicted == labels).float().sum()\n",
    "                total_predictions += torch.numel(labels)\n",
    "\n",
    "            validation_accuracy = correct_predictions / total_predictions\n",
    "            logging.info(f\"Validation Accuracy: {validation_accuracy.item()}\")\n",
    "            validation_accuracies.append(validation_accuracy.item())\n",
    "\n",
    "    return train_accuracies, validation_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m criterion \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss() \u001b[38;5;66;03m# loss function\u001b[39;00m\n\u001b[1;32m     31\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m) \u001b[38;5;66;03m# optimizer\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m train_accuracies, validation_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 55\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, validation_dataloader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/umich/w24/engr100/project3/env/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/umich/w24/engr100/project3/env/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# load data and make dataset\n",
    "npz_path = \"data.npz\"\n",
    "mel_dataset = FrameDataset(npz_path)\n",
    "\n",
    "print(\"split dataset\")\n",
    "# split dataset into training, validation, and testing\n",
    "# sizes of 80% 10% and 10%\n",
    "train_size = int(0.8 * len(mel_dataset))\n",
    "validation_size = int(0.1 * len(mel_dataset))\n",
    "test_size = len(mel_dataset) - train_size - validation_size\n",
    "\n",
    "# split data sets into their respective sizes\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    mel_dataset, [train_size, validation_size, test_size]\n",
    ")\n",
    "\n",
    "print(\"create dataloaders\")\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"make model\")\n",
    "# make model\n",
    "num_classes = 3\n",
    "model = MultiLabelCNN(num_classes)\n",
    "criterion = BCEWithLogitsLoss() # loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9) # optimizer\n",
    "\n",
    "print(\"training\")\n",
    "train_accuracies, validation_accuracies = train_model(\n",
    "    model, train_dataloader, validation_dataloader, criterion, optimizer, epochs=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
