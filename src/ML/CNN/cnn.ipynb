{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITON\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import ELU\n",
    "from torch import flatten\n",
    "\n",
    "class MultiLabelCNN(Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiLabelCNN, self).__init__()\n",
    "        \n",
    "        self.hidden_layers = Sequential (\n",
    "            Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3)),\n",
    "            BatchNorm2d(64),\n",
    "            ELU(),\n",
    "            MaxPool2d((2,2)),\n",
    "\n",
    "            Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3)),\n",
    "            BatchNorm2d(128),\n",
    "            ELU(),\n",
    "            MaxPool2d((2, 2)),\n",
    "\n",
    "            Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3)),\n",
    "            BatchNorm2d(256),\n",
    "            ELU(),\n",
    "            MaxPool2d((3, 3)),\n",
    "\n",
    "            Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3)),\n",
    "            BatchNorm2d(256),\n",
    "            ELU(),\n",
    "            MaxPool2d((3, 3)),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(in_features=256, out_features=128),  # Adjusted input features to match flattened conv output\n",
    "            ELU(),\n",
    "            Linear(in_features=128, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the convolutional layers\n",
    "        x = self.hidden_layers(x)\n",
    "        \n",
    "        # Flatten the output of the convolutional layers to fit linear layer input\n",
    "        x = flatten(x, 1)  # Flatten all dimensions except the batch\n",
    "        \n",
    "        # Pass data through linear layers\n",
    "        x = self.linear_layers(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wenze\\Desktop\\project_3\\src\\ML\\CNN\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# compress\n",
    "from utils import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "curr = \"../../../sample_audio_training/\"\n",
    "\n",
    "folders = ['oboe', 'trumpet', 'violin']\n",
    "files = []\n",
    "labels = []\n",
    "\n",
    "for folder in folders:\n",
    "    folderPath = os.path.join(curr, folder)\n",
    "    for filename in os.listdir(folderPath):\n",
    "        file_path = os.path.join(folderPath, filename)\n",
    "        if os.path.isfile(file_path):  # Make sure it's a file, not a directory or a symlink\n",
    "            files.append(file_path)\n",
    "            labels.append(folder)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "numeric_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "process_and_save_audio(files=files, labels=numeric_labels, output_path=\"data.npz\", sr=22050, add_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from typing import Tuple\n",
    "import logging, sys\n",
    "import numpy as np\n",
    "\n",
    "def load_npz_file_with_condition(file_path, max_size: int):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    if file_size > max_size:\n",
    "        logging.info(\n",
    "            f\"File size is {file_size / (1024**2):.2f}MB. Using mmap_mode='r'.\"\n",
    "        )\n",
    "        data = np.load(file_path, mmap_mode=\"r\", allow_pickle=True)\n",
    "    else:\n",
    "        logging.info(f\"File size is {file_size / (1024**2):.2f}MB. Loading normally.\")\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, npz_path):\n",
    "        self.data = load_npz_file_with_condition(npz_path, max_size=1024**3)\n",
    "        self.keys = [k for k in self.data.keys() if \"_data\" in k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    # return a 96 x 87 matrix for the CNN\n",
    "    def __getitem__(self, idx):\n",
    "        data_key = self.keys[idx]\n",
    "        data = self.data[data_key]\n",
    "        labels = self.data[f'{data_key.split(\"_data_\")[0]}_labels']\n",
    "        return torch.tensor(data.reshape(1, 96, 87), dtype=torch.float32), torch.tensor(\n",
    "            labels, dtype=torch.float32\n",
    "        )\n",
    "    \n",
    "def train_model(\n",
    "    model, train_dataloader, validation_dataloader, criterion, optimizer, epochs=5\n",
    ") -> Tuple[list, list]:\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logging.info(f\"Epoch {epoch+1}\")\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for data, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            correct_predictions += (predicted == labels).float().sum()\n",
    "            total_predictions += torch.numel(labels)\n",
    "\n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "        logging.info(f\"Loss: {total_loss}\")\n",
    "        logging.info(f\"Train Accuracy: {train_accuracy.item()}\")\n",
    "        train_accuracies.append(train_accuracy.item())\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            for data, labels in validation_dataloader:\n",
    "                outputs = model(data)\n",
    "                predicted = torch.sigmoid(outputs) > 0.5\n",
    "                correct_predictions += (predicted == labels).float().sum()\n",
    "                total_predictions += torch.numel(labels)\n",
    "\n",
    "            validation_accuracy = correct_predictions / total_predictions\n",
    "            logging.info(f\"Validation Accuracy: {validation_accuracy.item()}\")\n",
    "            validation_accuracies.append(validation_accuracy.item())\n",
    "\n",
    "    return train_accuracies, validation_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split dataset\n",
      "create dataloaders\n",
      "make model\n",
      "training\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# load data and make dataset\n",
    "npz_path = \"data.npz\"\n",
    "mel_dataset = FrameDataset(npz_path)\n",
    "\n",
    "print(\"split dataset\")\n",
    "# split dataset into training, validation, and testing\n",
    "# sizes of 80% 10% and 10%\n",
    "train_size = int(0.8 * len(mel_dataset))\n",
    "validation_size = int(0.1 * len(mel_dataset))\n",
    "test_size = len(mel_dataset) - train_size - validation_size\n",
    "\n",
    "# split data sets into their respective sizes\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    mel_dataset, [train_size, validation_size, test_size]\n",
    ")\n",
    "\n",
    "print(\"create dataloaders\")\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"make model\")\n",
    "# make model\n",
    "num_classes = 3\n",
    "model = MultiLabelCNN(num_classes)\n",
    "criterion = BCEWithLogitsLoss() # loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9) # optimizer\n",
    "\n",
    "print(\"training\")\n",
    "train_accuracies, validation_accuracies = train_model(\n",
    "    model, train_dataloader, validation_dataloader, criterion, optimizer, epochs=20\n",
    ")\n",
    "\n",
    "# save model\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
