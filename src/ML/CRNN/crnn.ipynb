{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRNN Base Structure (TODO Training and Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base structure of CRNN\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.features = nn.Sequential (\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        W = 86 // (2**4)\n",
    "        H = 96 // (2**4)\n",
    "        C = 256\n",
    "\n",
    "        totalInstruments = 3\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=C*H, \n",
    "            hidden_size=256, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # out_features is technically 18 in paper, but realistically 3 at the beginning\n",
    "        self.fc = nn.Linear(in_features=256, out_features=totalInstruments)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        conv_output = self.features(input)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "\n",
    "        fc_input = gru_output[:, -1, :]\n",
    "\n",
    "        return self.fc(fc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future stuff for training and accuracy of CNN\n",
    "import torch.optim as optim\n",
    "\n",
    "model = CRNN()\n",
    "lossAlg = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import logging, sys\n",
    "\n",
    "sys.path.append('/Users/adarshbharathwaj/Desktop/eng100/project3/ENGR_100_Project_3/src')\n",
    "from utils import *\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, npz_path):\n",
    "        self.data = load_npz_file_with_condition(npz_path, max_size=1024**3)\n",
    "        self.keys = [k for k in self.data.keys() if \"_data\" in k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_key = self.keys[idx]\n",
    "        data = self.data[data_key]\n",
    "        labels = self.data[f'{data_key.split(\"_data_\")[0]}_labels']\n",
    "        return torch.tensor(data.reshape(-1), dtype=torch.float32), torch.tensor(\n",
    "            labels, dtype=torch.float32\n",
    "        )\n",
    "    \n",
    "def load_npz_file_with_condition(file_path, max_size: int):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    if file_size > max_size:\n",
    "        logging.info(\n",
    "            f\"File size is {file_size / (1024**2):.2f}MB. Using mmap_mode='r'.\"\n",
    "        )\n",
    "        data = np.load(file_path, mmap_mode=\"r\", allow_pickle=True)\n",
    "    else:\n",
    "        logging.info(f\"File size is {file_size / (1024**2):.2f}MB. Loading normally.\")\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# def train_model(\n",
    "#     model, train_dataloader, validation_dataloader, criterion, optimizer, epochs=5\n",
    "# ) -> Tuple[list, list]:\n",
    "#     train_accuracies = []\n",
    "#     validation_accuracies = []\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         logging.info(f\"Epoch {epoch+1}\")\n",
    "#         model.train()  # Set model to training mode\n",
    "#         total_loss = 0\n",
    "#         correct_predictions = 0\n",
    "#         total_predictions = 0\n",
    "#         for data, labels in train_dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(data)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             predicted = torch.sigmoid(outputs) > 0.5\n",
    "#             correct_predictions += (predicted == labels).float().sum()\n",
    "#             total_predictions += torch.numel(labels)\n",
    "\n",
    "#         train_accuracy = correct_predictions / total_predictions\n",
    "#         logging.info(f\"Loss: {total_loss}\")\n",
    "#         logging.info(f\"Train Accuracy: {train_accuracy.item()}\")\n",
    "#         train_accuracies.append(train_accuracy.item())\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()  # Set model to evaluation mode\n",
    "#         with torch.no_grad():\n",
    "#             correct_predictions = 0\n",
    "#             total_predictions = 0\n",
    "#             for data, labels in validation_dataloader:\n",
    "#                 outputs = model(data)\n",
    "#                 predicted = torch.sigmoid(outputs) > 0.5\n",
    "#                 correct_predictions += (predicted == labels).float().sum()\n",
    "#                 total_predictions += torch.numel(labels)\n",
    "\n",
    "#             validation_accuracy = correct_predictions / total_predictions\n",
    "#             logging.info(f\"Validation Accuracy: {validation_accuracy.item()}\")\n",
    "#             validation_accuracies.append(validation_accuracy.item())\n",
    "\n",
    "#     return train_accuracies, validation_accuracies\n",
    "\n",
    "\n",
    "# def plot_accuracy(\n",
    "#     train_accuracies: list, validation_accuracies: list, epoch_count: int\n",
    "# ):\n",
    "#     epochs = range(1, epoch_count + 1)\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(epochs, train_accuracies, label=\"Training Accuracy\")\n",
    "#     plt.plot(epochs, validation_accuracies, label=\"Validation Accuracy\")\n",
    "#     plt.title(\"Training and Validation Accuracy\")\n",
    "#     plt.xlabel(\"Epoch\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def test_model(model, test_dataloader):\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         correct_predictions = 0\n",
    "#         total_predictions = 0\n",
    "#         for data, labels in test_dataloader:\n",
    "#             outputs = model(data)\n",
    "#             predicted = torch.sigmoid(outputs) > 0.5\n",
    "#             correct_predictions += (predicted == labels).float().sum()\n",
    "#             total_predictions += torch.numel(labels)\n",
    "#         logging.info(\n",
    "#             f\"Test Accuracy: {(correct_predictions / total_predictions).item()}\"\n",
    "#         )\n",
    "\n",
    "\n",
    "# def save_model(model, path: str):\n",
    "#     torch.save(model.state_dict(), path)\n",
    "#     logging.info(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "# def load_model(*parameters, path: str):\n",
    "#     loaded_model = MultiLabelMLP(parameters)\n",
    "\n",
    "#     # Then, load the saved state dict\n",
    "#     loaded_model.load_state_dict(torch.load(path))\n",
    "\n",
    "#     return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CRNN.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28minput\u001b[39m, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 24\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mCRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lossAlg(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mTypeError\u001b[0m: CRNN.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "npz_path = \"data.npz\"\n",
    "full_dataset = FrameDataset(npz_path)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "validation_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - validation_size\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, validation_size, test_size]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        input, labels = batch\n",
    "        output = CRNN(input)\n",
    "        loss = lossAlg(output.view(-1), labels.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
