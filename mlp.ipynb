{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import logging, sys\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(MultiLabelMLP, self).__init__()\n",
    "\n",
    "        # Initialize the ModuleList for hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        # The dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Create the first hidden layer\n",
    "        self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "\n",
    "        # Create subsequent hidden layers\n",
    "        for h1, h2 in zip(hidden_sizes[:-1], hidden_sizes[1:]):\n",
    "            self.hidden_layers.append(nn.Linear(h1, h2))\n",
    "\n",
    "        # Create the output layer\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each hidden layer with ReLU activation and dropout\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)  # Apply dropout after activation\n",
    "\n",
    "        # Apply the output layer\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_file_with_condition(file_path, max_size: int):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    if file_size > max_size:\n",
    "        logging.info(\n",
    "            f\"File size is {file_size / (1024**2):.2f}MB. Using mmap_mode='r'.\"\n",
    "        )\n",
    "        data = np.load(file_path, mmap_mode=\"r\", allow_pickle=True)\n",
    "    else:\n",
    "        logging.info(f\"File size is {file_size / (1024**2):.2f}MB. Loading normally.\")\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, npz_path):\n",
    "        self.data = load_npz_file_with_condition(npz_path, max_size=1024**3)\n",
    "        self.keys = [k for k in self.data.keys() if \"_data\" in k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_key = self.keys[idx]\n",
    "        data = self.data[data_key]\n",
    "        labels = self.data[f'{data_key.split(\"_data_\")[0]}_labels']\n",
    "        return torch.tensor(data.reshape(-1), dtype=torch.float32), torch.tensor(\n",
    "            labels, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_dataloader, validation_dataloader, criterion, optimizer, epochs=5\n",
    ") -> Tuple[list, list]:\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logging.info(f\"Epoch {epoch+1}\")\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for data, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            correct_predictions += (predicted == labels).float().sum()\n",
    "            total_predictions += torch.numel(labels)\n",
    "\n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "        logging.info(f\"Loss: {total_loss}\")\n",
    "        logging.info(f\"Train Accuracy: {train_accuracy.item()}\")\n",
    "        train_accuracies.append(train_accuracy.item())\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            for data, labels in validation_dataloader:\n",
    "                outputs = model(data)\n",
    "                predicted = torch.sigmoid(outputs) > 0.5\n",
    "                correct_predictions += (predicted == labels).float().sum()\n",
    "                total_predictions += torch.numel(labels)\n",
    "\n",
    "            validation_accuracy = correct_predictions / total_predictions\n",
    "            logging.info(f\"Validation Accuracy: {validation_accuracy.item()}\")\n",
    "            validation_accuracies.append(validation_accuracy.item())\n",
    "\n",
    "    return train_accuracies, validation_accuracies\n",
    "\n",
    "\n",
    "def plot_accuracy(\n",
    "    train_accuracies: list, validation_accuracies: list, epoch_count: int\n",
    "):\n",
    "    epochs = range(1, epoch_count + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\")\n",
    "    plt.plot(epochs, validation_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_model(model, test_dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for data, labels in test_dataloader:\n",
    "            outputs = model(data)\n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            correct_predictions += (predicted == labels).float().sum()\n",
    "            total_predictions += torch.numel(labels)\n",
    "        logging.info(\n",
    "            f\"Test Accuracy: {(correct_predictions / total_predictions).item()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def save_model(model, path: str):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    logging.info(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(*parameters, path: str):\n",
    "    loaded_model = MultiLabelMLP(parameters)\n",
    "\n",
    "    # Then, load the saved state dict\n",
    "    loaded_model.load_state_dict(torch.load(path))\n",
    "\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming MultiLabelMLP is defined elsewhere\n",
    "\n",
    "npz_path = \"processed_audio.npz\"\n",
    "full_dataset = FrameDataset(npz_path)\n",
    "\n",
    "# Split dataset into training, validation, and testing\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "validation_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - validation_size\n",
    "batch_size = 20\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, validation_size, test_size]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define model, criterion, and optimizer\n",
    "input_size = 96 * 87  # Flattened frame size\n",
    "hidden_sizes = [256, 256, 256, 256]  # Example hidden layer sizes\n",
    "num_classes = 3  # Adjust based on your label dimensionality\n",
    "model = MultiLabelMLP(input_size, hidden_sizes, num_classes)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "train_accuracies, validation_accuracies = train_model(\n",
    "    model, train_dataloader, validation_dataloader, criterion, optimizer, epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_dataloader)\n",
    "save_model(model, \"pretrained_models/MLP/MLP-1_0_0.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
